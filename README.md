# NLP_MovieReviews

Natural language processing utilizes text to create a representation or understanding of the words for computers. Because computers have no semantic understanding, the text must be processed and encoded as vectors to carry that meaning with them. The corpus includes reviews from 25 movies, consisting of six action, horror, sci-fi and seven comedy movies. There were five positive and five negative reviews collected for each of the movies, resulting in a total of 250 documents in the corpus. The tokens within the document were extracted and preprocessed before adding it to the vocabulary. The goal is to develop a corpus vocabulary that encapsulates the meaningful and important terms that would best represent the documents, so that they can be clustered with their corresponding genre and sentiment. 

### Assignment 1: First Vectorized Representation
To accomplish this goal, different data wrangling methods (lemmatization, stemming, removing stopwords, etc.) will be experimented to see which best processes the texts for the intended purpose. Assignment 1 experiments with different vectorization methods including tf-idf, Word2Vec, and Doc2Vec models. With these methods, the similarity between tokens and documents can be observed. The similarity will show if the tokens or documents are able to be clustered and classified with one another. This assignment emphasized the importance of data wrangling before vectorizing as it affects the weight of each token. 

### Assignment 2: Assess Clustering and Classification Outputs
In Assignment 2, the goal is to group similar documents into clusters. This objective can be accomplished through clustering, sentiment analysis modeling, and topic modeling. Clustering is an unsupervised learning method that clusters similar documents together based on similar features, based off the vectorization of the documents. Sentiment analysis modeling can be created with supervised learning methods such as support vector machines, naïve bayes, logistic regression, random forest classifier to classify the documents into positive or negative sentiment. These models use the numerical representation of the documents as the inputs to train the classification model. Topic modeling will use latent semantic analysis, LSA, as well as latent dirichlet allocation, LDA, models to uncover latent variables to represent the topics of the clusters. These topics and its associated terms will be used to identify similar documents.

The k-means clustering did the best job with grouping similar documents together. In particular, the k-means clustering with 24 clusters provided the clusters that made the most sense and had the highest silhouette score, which indicates there were fewer overlapping clusters. The sentiment analysis models using support vector machines, naïve bayes, logistic regression, and random forest classifier all did not do a good job in discriminating between the positive and negative sentiment of the documents. Their accuracy scores for both versions, tf-idf and Doc2Vec, were less 0.50, indicating that a random guess between the two sentiments would have yielded better results than the models. For topic modeling, the LSA model had better coherence scores than LDA. The LSA model with 4 topics did the best in finding the latent similarities between documents.  From the topic modeling, it was evident that the data needed to be cleaned more to take out common and meaningless terms such as “character” and “story” to reduce the noise in topic modeling as well as the other clustering and classification methods.

### Assignment 3: Ontology Plus Context and Modeling
The representation of text is the most important part of natural language processing. The issue is that often the text is not represented with enough meaning, which results in a lack of poor results in applications such as clustering and classification. The use of ontologies can help model the concepts and ideas behind the words and sentences in a structured way so that the relationships between the concepts can be observed. The goal is to build an ontology based on the topic of ten documents of Guardians of the Galaxy movie reviews. An ontology is created manually, using the ontology editor Protégé, and using Python algorithms. 

The entire corpus of 250 movie review documents, with half being positive reviews and the other half being negative reviews, is then used to create a deep learning model that can classify the documents into its sentiment class. A long short term memory model will be built and used to capture the sequential correlations, which will be stored in the memory to be used when iterating through the next time step. The goal of this model is to use the text to discriminate between the two sentiment classes.

Each method displayed relationships to the topic, providing more context into the documents and their common themes and how those relate to each other. The use of knowledge graphs also provided a way to observe entity relationships where the documents had a few entities in common and showed coreferences that may need to be addressed. The LSTM architecture was used to keep the temporal correlations of the sequence of words in its memory so that it can provide context into the predicted output.
